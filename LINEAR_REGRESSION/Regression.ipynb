{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "266cb2ee",
   "metadata": {},
   "source": [
    "# üìä Regression from Scratch  \n",
    "\n",
    "This notebook demonstrates how to build **linear regression models step by step, without relying on machine learning libraries**.  \n",
    "The goal is to understand the mathematics and algorithms behind regression by directly implementing them in Python and NumPy.  \n",
    "\n",
    "We will cover:  \n",
    "- Ordinary Least Squares (OLS) regression  \n",
    "- Coordinate descent updates  \n",
    "- The role of residuals and projections  \n",
    "- Extensions to regularized regression (Lasso, Ridge)  \n",
    "\n",
    "By the end, you‚Äôll have a clear intuition of how regression works \"under the hood\", beyond just calling `sklearn.LinearRegression`.  \n",
    "\n",
    "---\n",
    "\n",
    "üë®‚Äçüíª *Written by:* **Samuel Boluwatife Giwa**  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc28b6c",
   "metadata": {},
   "source": [
    "The solution to OLS has a closed-form expression:\n",
    "\n",
    "$$\n",
    "\\hat{\\beta} = (X^\\top X)^{-1} X^\\top y\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ebb411f",
   "metadata": {},
   "source": [
    "Once the coefficients are estimated, predictions are given by:\n",
    "\n",
    "$$\n",
    "\\hat{y} = X \\hat{\\beta}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ff62173",
   "metadata": {},
   "source": [
    "## üîπ Coefficient estimation using the Coordinate Descent Idea  \n",
    "\n",
    "Instead of updating all coefficients at once, **we update one coordinate (one parameter) at a time**, cycling through them:  \n",
    "\n",
    "1. **Fix all coefficients except** $ \\beta_j $.  \n",
    "2. **Minimize the cost function** with respect to only $ \\beta_j $. \n",
    "3. **Move to the next coordinate**, and repeat.  \n",
    "\n",
    "üëâ So, instead of taking a ‚Äúbig step‚Äù in \\( p \\)-dimensional space, we take **1D steps along coordinate axes**.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa4d5cab",
   "metadata": {},
   "source": [
    "## üîπ Coordinate Descent Update Rule  \n",
    "\n",
    "For each parameter $ \\beta_j $, keeping all others fixed:\n",
    "\n",
    "1. **Partial residual:**\n",
    "\n",
    "$$\n",
    "r_j = y - X \\beta + X_j \\beta_j\n",
    "$$\n",
    "\n",
    "2. **Correlation with residual:**\n",
    "\n",
    "$$\n",
    "\\tilde{\\beta}_j = \\frac{1}{n} X_j^\\top r_j\n",
    "$$\n",
    "\n",
    "3. **Normalization factor:**\n",
    "\n",
    "$$\n",
    "a_j = \\frac{1}{n} X_j^\\top X_j\n",
    "$$\n",
    "\n",
    "4. **Update coefficient:**\n",
    "\n",
    "- **OLS (no penalty):**\n",
    "\n",
    "$$\n",
    "\\beta_j \\leftarrow \\frac{1}{a_j} \\, \\tilde{\\beta}_j\n",
    "$$\n",
    "\n",
    "- **Lasso (with L1 penalty):**\n",
    "\n",
    "$$\n",
    "\\beta_j \\leftarrow \\frac{1}{a_j} \\, S(\\tilde{\\beta}_j, \\lambda)\n",
    "$$\n",
    "\n",
    "where  \n",
    "\n",
    "$$\n",
    "S(z, \\lambda) = \\text{sign}(z) \\cdot \\max(|z| - \\lambda, \\, 0)\n",
    "$$\n",
    "\n",
    "is the **soft-thresholding operator**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f3624eb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True coefficients: [ 2.  -3.5  1. ]\n",
      "Estimated coefficients: [ 1.89943376 -3.57613286  0.9505738 ]\n",
      "Intercept: 0.062398939476852266\n",
      "Train MSE: 0.192187794681832\n",
      "Test MSE: 0.19902017464291\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ORDINARY LEAST SQUARE\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "# Synthetic Dataset\n",
    "np.random.seed(42)\n",
    "\n",
    "n, p = 100, 3   # 100 samples, 3 features\n",
    "X = np.random.randn(n, p)   # random features\n",
    "true_beta = np.array([2.0, -3.5, 1.0])  # true coefficients\n",
    "y = X @ true_beta + 0.5 * np.random.randn(n)  \n",
    "\n",
    "\n",
    "# Train/Test Split\n",
    "indices = np.arange(n)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_size = int(0.7 * n)\n",
    "train_idx, test_idx = indices[:train_size], indices[train_size:]\n",
    "\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_test, y_test   = X[test_idx], y[test_idx]\n",
    "\n",
    "n_train, _ = X_train.shape\n",
    "\n",
    "# Training via Coordinate Descent (OLS)\n",
    "beta = np.zeros(p)\n",
    "iter_ = 200\n",
    "\n",
    "for _ in range(iter_):\n",
    "    for j in range(p):\n",
    "        # partial residual\n",
    "        r_j = y_train - X_train @ beta + X_train[:, j] * beta[j]\n",
    "        \n",
    "        # correlation\n",
    "        beta_tilde = (1/n_train) * np.dot(X_train[:, j], r_j)\n",
    "        \n",
    "        a_j = (1/n_train) * np.dot(X_train[:, j], X_train[:, j])\n",
    "        \n",
    "    \n",
    "        beta[j] = (1/a_j) * beta_tilde\n",
    "\n",
    "# Intercept\n",
    "intercept_ = y_train.mean() - np.dot(X_train.mean(axis=0), beta)\n",
    "\n",
    "# Predictions\n",
    "y_train_pred = X_train @ beta + intercept_\n",
    "y_test_pred  = X_test @ beta + intercept_\n",
    "\n",
    "\n",
    "# Mean Squared Error\n",
    "mse_train = np.mean((y_train - y_train_pred)**2)\n",
    "mse_test  = np.mean((y_test - y_test_pred)**2)\n",
    "\n",
    "print(\"True coefficients:\", true_beta)\n",
    "print(\"Estimated coefficients:\", beta)\n",
    "print(\"Intercept:\", intercept_)\n",
    "print(\"Train MSE:\", mse_train)\n",
    "print(\"Test MSE:\", mse_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3bff2246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true coefficients: [ 2.  -3.5  1. ]\n",
      "Estimated coefficients: [ 1.20034496 -3.20705442  0.55732047]\n",
      "Intercept: 0.30701451085180265\n",
      "Test MSE: 0.9109100636250651\n",
      "Test R2: 0.9350604084826118\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "\n",
    "# LASSO REGRESSION\n",
    "def S(z, lam):\n",
    "    return np.sign(z) * np.maximum(np.abs(z) - lam, 0.0)\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n, p = 100, 3\n",
    "X = np.random.randn(n, p)\n",
    "true_beta = np.array([2.0, -3.5, 1.0])\n",
    "y = X @ true_beta + 0.5 * np.random.randn(n)\n",
    "\n",
    "indices = np.arange(n)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_size = int(0.7 * n)\n",
    "train_idx, test_idx = indices[:train_size], indices[train_size:]\n",
    "\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_test, y_test   = X[test_idx], y[test_idx]\n",
    "\n",
    "n_train, p = X_train.shape\n",
    "beta = np.zeros(p)\n",
    "lam = 0.5\n",
    "iter_ = 100\n",
    "\n",
    "for _ in range(iter_):\n",
    "    for j in range(p):\n",
    "        r_j = y_train - X_train @ beta + X_train[:, j] * beta[j]\n",
    "        beta_tilde = (1/n_train) * np.dot(X_train[:, j], r_j)\n",
    "        a_j = (1/n_train) * np.dot(X_train[:, j], X_train[:, j])\n",
    "        beta[j] = (1/a_j) * S(beta_tilde, lam)\n",
    "\n",
    "intercept_ = y_train.mean() - np.dot(X_train.mean(axis=0), beta)\n",
    "\n",
    "y_pred = X_test @ beta + intercept_\n",
    "\n",
    "mse = np.mean((y_test - y_pred) ** 2)\n",
    "r2 = 1 - np.sum((y_test - y_pred) ** 2) / np.sum((y_test - y_test.mean()) ** 2)\n",
    "\n",
    "print(\"true coefficients:\", true_beta)\n",
    "print(\"Estimated coefficients:\", beta)\n",
    "print(\"Intercept:\", intercept_)\n",
    "print(\"Test MSE:\", mse)\n",
    "print(\"Test R2:\", r2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "576df1c0",
   "metadata": {},
   "source": [
    "# Ridge Regression: Coordinate Descent Update\n",
    "\n",
    "### 1. Objective Function\n",
    "Ridge regression solves\n",
    "\n",
    "$$\n",
    "\\min_{\\beta}\\; \\frac{1}{2n}\\|y - X\\beta\\|_2^2 + \\frac{\\lambda}{2}\\|\\beta\\|_2^2,\n",
    "$$\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "- $n$: number of samples  \n",
    "- $p$: number of features  \n",
    "- $X \\in \\mathbb{R}^{n \\times p}$: design matrix  \n",
    "- $y \\in \\mathbb{R}^{n}$: response vector  \n",
    "- $\\beta \\in \\mathbb{R}^{p}$: coefficient vector  \n",
    "- $\\lambda \\geq 0$: regularization parameter  \n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Coordinate Descent Setup\n",
    "When updating one coordinate $ \\beta_j $ at a time (others fixed), define the **partial residual**\n",
    "\n",
    "$$\n",
    "r_j = y - \\sum_{k\\ne j} X_k \\beta_k = y - X\\beta + X_j\\beta_j,\n",
    "$$\n",
    "\n",
    "where $ X_j $ denotes the \\(j\\)-th column of $ X $.  \n",
    "\n",
    "---\n",
    "\n",
    "### 3. Loss as a Function of $ \\beta_j $\n",
    "The objective restricted to $ \\beta_j$ is\n",
    "\n",
    "$$\n",
    "L(\\beta_j) = \\frac{1}{2n}\\|r_j - X_j\\beta_j\\|_2^2 + \\frac{\\lambda}{2}\\beta_j^2.\n",
    "$$\n",
    "\n",
    "Expanding,\n",
    "\n",
    "$$\n",
    "L(\\beta_j) = \\frac{1}{2n}\\!\\left(r_j^T r_j - 2\\beta_j X_j^T r_j + \\beta_j^2 X_j^T X_j\\right) + \\frac{\\lambda}{2}\\beta_j^2.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 4. Derivative and Optimality Condition\n",
    "Differentiate w.r.t. \\(\\beta_j\\):\n",
    "\n",
    "$$\n",
    "\\frac{\\partial L}{\\partial \\beta_j} = -\\frac{1}{n}X_j^T r_j + \\frac{1}{n}\\beta_j X_j^T X_j + \\lambda \\beta_j.\n",
    "$$\n",
    "\n",
    "Set the derivative to zero:\n",
    "\n",
    "$$\n",
    "-\\frac{1}{n}X_j^T r_j + \\left(\\frac{1}{n}X_j^T X_j + \\lambda\\right)\\beta_j = 0.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "### 5. Coordinate Update Rule\n",
    "Solve for $ \\beta_j $:\n",
    "\n",
    "$$\n",
    "\\beta_j = \\frac{\\frac{1}{n} X_j^T r_j}{\\frac{1}{n} X_j^T X_j + \\lambda}.\n",
    "$$\n",
    "\n",
    "If features are standardized so that $\\left(\\frac{1}{n} X_j^{T} X_j = 1\\right)$, this simplifies to\n",
    "\n",
    "$$\n",
    "\\beta_j = \\frac{\\frac{1}{n} X_j^T r_j}{1 + \\lambda}.\n",
    "$$\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8e85c1e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True Beta: [ 2.  -3.5  1. ]\n",
      "Beta: [ 1.19661734 -2.47528447  0.72070361]\n",
      "Intercept: 0.4120446108206335\n",
      "Train MSE: 1.8920924188714143  Test MSE: 1.5336362760345037\n",
      "Train R^2: 0.8928163011957736  Test R^2: 0.8906657009523147\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(42)\n",
    "\n",
    "n, p = 100, 3\n",
    "X = np.random.randn(n, p)\n",
    "true_beta = np.array([2.0, -3.5, 1.0])\n",
    "y = X @ true_beta + 0.5 * np.random.randn(n)\n",
    "\n",
    "indices = np.arange(n)\n",
    "np.random.shuffle(indices)\n",
    "\n",
    "train_size = int(0.7 * n)\n",
    "train_idx, test_idx = indices[:train_size], indices[train_size:]\n",
    "\n",
    "X_train, y_train = X[train_idx], y[train_idx]\n",
    "X_test, y_test   = X[test_idx], y[test_idx]\n",
    "\n",
    "n_train, p = X_train.shape\n",
    "beta = np.zeros(p)\n",
    "lam = 0.5\n",
    "iter_ = 100\n",
    "\n",
    "for _ in range(iter_):\n",
    "    for j in range(p):\n",
    "        r_j = y_train - X_train @ beta + X_train[:, j] * beta[j]\n",
    "        beta_tilde = (1/n_train) * np.dot(X_train[:, j], r_j)\n",
    "        a_j = (1/n_train) * np.dot(X_train[:, j], X_train[:, j]) + lam\n",
    "        beta[j] = beta_tilde / a_j\n",
    "\n",
    "intercept_ = y_train.mean() - np.dot(X_train.mean(axis=0), beta)\n",
    "\n",
    "y_train_pred = X_train @ beta + intercept_\n",
    "y_test_pred = X_test @ beta + intercept_\n",
    "\n",
    "mse_train = np.mean((y_train - y_train_pred) ** 2)\n",
    "mse_test = np.mean((y_test - y_test_pred) ** 2)\n",
    "\n",
    "r2_train = 1 - np.sum((y_train - y_train_pred) ** 2) / np.sum((y_train - y_train.mean()) ** 2)\n",
    "r2_test = 1 - np.sum((y_test - y_test_pred) ** 2) / np.sum((y_test - y_test.mean()) ** 2)\n",
    "\n",
    "print(\"True Beta:\", true_beta)\n",
    "print(\"Beta:\", beta)\n",
    "print(\"Intercept:\", intercept_)\n",
    "print(\"Train MSE:\", mse_train, \" Test MSE:\", mse_test)\n",
    "print(\"Train R^2:\", r2_train, \" Test R^2:\", r2_test)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
