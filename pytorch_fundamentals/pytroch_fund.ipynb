{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ea8efe6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 5467.289271004303\n",
      "199 3666.3649771824325\n",
      "299 2461.4321968336185\n",
      "399 1654.7125915914087\n",
      "499 1114.2225904621623\n",
      "599 751.8369899821392\n",
      "699 508.6807405669347\n",
      "799 345.3965263076051\n",
      "899 235.65772839843214\n",
      "999 161.84244680790704\n",
      "1099 112.14717065080447\n",
      "1199 78.65995198560495\n",
      "1299 56.07338390841293\n",
      "1399 40.82442289594058\n",
      "1499 30.519132340876386\n",
      "1599 23.547719960620064\n",
      "1699 18.82674430340552\n",
      "1799 15.626358143969696\n",
      "1899 13.454452190300469\n",
      "1999 11.978898012928724\n",
      "Result: y = -0.04035710108594503 + 0.8165658173546284 x + 0.006962271482386262 x^2 + -0.08761584147925086 x^3\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Create random input and output data\n",
    "x = np.linspace(-math.pi, math.pi, 2000)\n",
    "y = np.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = np.random.randn()\n",
    "b = np.random.randn()\n",
    "c = np.random.randn()\n",
    "d = np.random.randn()\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    # y = a + b x + c x^2 + d x^3\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = np.square(y_pred - y).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "print(f'Result: y = {a} + {b} x + {c} x^2 + {d} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d12286",
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\") # Uncomment this to run on GPU\n",
    "\n",
    "# Create random input and output data\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Randomly initialize weights\n",
    "a = torch.randn((), device=device, dtype=dtype)\n",
    "b = torch.randn((), device=device, dtype=dtype)\n",
    "c = torch.randn((), device=device, dtype=dtype)\n",
    "d = torch.randn((), device=device, dtype=dtype)\n",
    "\n",
    "learning_rate = 1e-6\n",
    "for t in range(2000):\n",
    "    # Forward pass: compute predicted y\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum().item()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss)\n",
    "\n",
    "    # Backprop to compute gradients of a, b, c, d with respect to loss\n",
    "    grad_y_pred = 2.0 * (y_pred - y)\n",
    "    grad_a = grad_y_pred.sum()\n",
    "    grad_b = (grad_y_pred * x).sum()\n",
    "    grad_c = (grad_y_pred * x ** 2).sum()\n",
    "    grad_d = (grad_y_pred * x ** 3).sum()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    a -= learning_rate * grad_a\n",
    "    b -= learning_rate * grad_b\n",
    "    c -= learning_rate * grad_c\n",
    "    d -= learning_rate * grad_d\n",
    "\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "30ff79d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration t =   99  loss(t)/loss(0) =   0.023018  a =   0.958863  b =   0.600468  c =   0.654799  d =   0.297076\n",
      "Iteration t =  199  loss(t)/loss(0) =   0.001825  a =   0.965667  b =   0.804914  c =   0.621384  d =   0.402361\n",
      "Iteration t =  299  loss(t)/loss(0) =   0.001021  a =   0.973981  b =   0.845220  c =   0.598300  d =   0.405436\n",
      "Iteration t =  399  loss(t)/loss(0) =   0.000810  a =   0.980062  b =   0.859392  c =   0.581521  d =   0.393434\n",
      "Iteration t =  499  loss(t)/loss(0) =   0.000671  a =   0.984485  b =   0.868984  c =   0.569317  d =   0.379885\n",
      "Iteration t =  599  loss(t)/loss(0) =   0.000568  a =   0.987703  b =   0.877386  c =   0.560440  d =   0.366846\n",
      "Iteration t =  699  loss(t)/loss(0) =   0.000487  a =   0.990043  b =   0.885158  c =   0.553984  d =   0.354589\n",
      "Iteration t =  799  loss(t)/loss(0) =   0.000421  a =   0.991745  b =   0.892417  c =   0.549288  d =   0.343111\n",
      "Iteration t =  899  loss(t)/loss(0) =   0.000367  a =   0.992983  b =   0.899206  c =   0.545872  d =   0.332371\n",
      "Iteration t =  999  loss(t)/loss(0) =   0.000320  a =   0.993884  b =   0.905558  c =   0.543387  d =   0.322321\n",
      "Iteration t = 1099  loss(t)/loss(0) =   0.000280  a =   0.994539  b =   0.911502  c =   0.541580  d =   0.312918\n",
      "Iteration t = 1199  loss(t)/loss(0) =   0.000246  a =   0.995015  b =   0.917063  c =   0.540266  d =   0.304120\n",
      "Iteration t = 1299  loss(t)/loss(0) =   0.000216  a =   0.995362  b =   0.922266  c =   0.539310  d =   0.295888\n",
      "Iteration t = 1399  loss(t)/loss(0) =   0.000190  a =   0.995614  b =   0.927135  c =   0.538615  d =   0.288185\n",
      "Iteration t = 1499  loss(t)/loss(0) =   0.000167  a =   0.995797  b =   0.931690  c =   0.538109  d =   0.280978\n",
      "Iteration t = 1599  loss(t)/loss(0) =   0.000147  a =   0.995931  b =   0.935953  c =   0.537741  d =   0.274235\n",
      "Iteration t = 1699  loss(t)/loss(0) =   0.000130  a =   0.996028  b =   0.939941  c =   0.537473  d =   0.267925\n",
      "Iteration t = 1799  loss(t)/loss(0) =   0.000114  a =   0.996098  b =   0.943673  c =   0.537279  d =   0.262021\n",
      "Iteration t = 1899  loss(t)/loss(0) =   0.000101  a =   0.996149  b =   0.947164  c =   0.537137  d =   0.256497\n",
      "Iteration t = 1999  loss(t)/loss(0) =   0.000089  a =   0.996186  b =   0.950431  c =   0.537035  d =   0.251328\n",
      "Iteration t = 2099  loss(t)/loss(0) =   0.000079  a =   0.996213  b =   0.953488  c =   0.536960  d =   0.246492\n",
      "Iteration t = 2199  loss(t)/loss(0) =   0.000070  a =   0.996233  b =   0.956349  c =   0.536906  d =   0.241967\n",
      "Iteration t = 2299  loss(t)/loss(0) =   0.000062  a =   0.996247  b =   0.959025  c =   0.536866  d =   0.237733\n",
      "Iteration t = 2399  loss(t)/loss(0) =   0.000055  a =   0.996258  b =   0.961529  c =   0.536837  d =   0.233771\n",
      "Iteration t = 2499  loss(t)/loss(0) =   0.000049  a =   0.996265  b =   0.963872  c =   0.536817  d =   0.230064\n",
      "Iteration t = 2599  loss(t)/loss(0) =   0.000044  a =   0.996271  b =   0.966064  c =   0.536802  d =   0.226596\n",
      "Iteration t = 2699  loss(t)/loss(0) =   0.000039  a =   0.996275  b =   0.968115  c =   0.536790  d =   0.223351\n",
      "Iteration t = 2799  loss(t)/loss(0) =   0.000035  a =   0.996278  b =   0.970035  c =   0.536783  d =   0.220314\n",
      "Iteration t = 2899  loss(t)/loss(0) =   0.000032  a =   0.996280  b =   0.971830  c =   0.536777  d =   0.217473\n",
      "Iteration t = 2999  loss(t)/loss(0) =   0.000029  a =   0.996282  b =   0.973511  c =   0.536771  d =   0.214815\n",
      "Iteration t = 3099  loss(t)/loss(0) =   0.000026  a =   0.996282  b =   0.975083  c =   0.536770  d =   0.212327\n",
      "Iteration t = 3199  loss(t)/loss(0) =   0.000024  a =   0.996282  b =   0.976554  c =   0.536770  d =   0.210000\n",
      "Iteration t = 3299  loss(t)/loss(0) =   0.000022  a =   0.996282  b =   0.977930  c =   0.536770  d =   0.207823\n",
      "Iteration t = 3399  loss(t)/loss(0) =   0.000020  a =   0.996282  b =   0.979218  c =   0.536770  d =   0.205785\n",
      "Iteration t = 3499  loss(t)/loss(0) =   0.000018  a =   0.996282  b =   0.980423  c =   0.536770  d =   0.203878\n",
      "Iteration t = 3599  loss(t)/loss(0) =   0.000017  a =   0.996282  b =   0.981551  c =   0.536770  d =   0.202095\n",
      "Iteration t = 3699  loss(t)/loss(0) =   0.000016  a =   0.996282  b =   0.982606  c =   0.536770  d =   0.200426\n",
      "Iteration t = 3799  loss(t)/loss(0) =   0.000015  a =   0.996282  b =   0.983593  c =   0.536770  d =   0.198864\n",
      "Iteration t = 3899  loss(t)/loss(0) =   0.000014  a =   0.996282  b =   0.984517  c =   0.536770  d =   0.197403\n",
      "Iteration t = 3999  loss(t)/loss(0) =   0.000013  a =   0.996282  b =   0.985381  c =   0.536770  d =   0.196035\n",
      "Iteration t = 4099  loss(t)/loss(0) =   0.000012  a =   0.996282  b =   0.986189  c =   0.536770  d =   0.194756\n",
      "Iteration t = 4199  loss(t)/loss(0) =   0.000011  a =   0.996282  b =   0.986946  c =   0.536770  d =   0.193559\n",
      "Iteration t = 4299  loss(t)/loss(0) =   0.000011  a =   0.996282  b =   0.987654  c =   0.536770  d =   0.192439\n",
      "Iteration t = 4399  loss(t)/loss(0) =   0.000010  a =   0.996282  b =   0.988316  c =   0.536770  d =   0.191391\n",
      "Iteration t = 4499  loss(t)/loss(0) =   0.000010  a =   0.996282  b =   0.988936  c =   0.536770  d =   0.190411\n",
      "Iteration t = 4599  loss(t)/loss(0) =   0.000010  a =   0.996282  b =   0.989516  c =   0.536770  d =   0.189493\n",
      "Iteration t = 4699  loss(t)/loss(0) =   0.000009  a =   0.996282  b =   0.990058  c =   0.536770  d =   0.188635\n",
      "Iteration t = 4799  loss(t)/loss(0) =   0.000009  a =   0.996282  b =   0.990566  c =   0.536770  d =   0.187832\n",
      "Iteration t = 4899  loss(t)/loss(0) =   0.000009  a =   0.996282  b =   0.991041  c =   0.536770  d =   0.187080\n",
      "Iteration t = 4999  loss(t)/loss(0) =   0.000009  a =   0.996282  b =   0.991486  c =   0.536770  d =   0.186377\n",
      "Result: y = 0.9962821006774902 + 0.991489827632904 x + 0.5367701649665833 x^2 + 0.18637022376060486 x^3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "# We want to be able to train our model on an `accelerator <https://pytorch.org/docs/stable/torch.html#accelerators>`__\n",
    "# such as CUDA, MPS, MTIA, or XPU. If the current accelerator is available, we will use it. Otherwise, we use the CPU.\n",
    "\n",
    "dtype = torch.float\n",
    "#device = torch.accelerator.current_accelerator().type if torch.accelerator.is_available() else \"cpu\"\n",
    "#print(f\"Using {device} device\")\n",
    "#torch.set_default_device(device)\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-1, 1, 2000, dtype=dtype)\n",
    "y = torch.exp(x) # A Taylor expansion would be 1 + x + (1/2) x**2 + (1/3!) x**3 + ...\n",
    "\n",
    "# Create random Tensors for weights. For a third order polynomial, we need\n",
    "# 4 weights: y = a + b x + c x^2 + d x^3\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "a = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "b = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "c = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "d = torch.randn((), dtype=dtype, requires_grad=True)\n",
    "\n",
    "initial_loss = 1.\n",
    "learning_rate = 1e-5\n",
    "for t in range(5000):\n",
    "    # Forward pass: compute predicted y using operations on Tensors.\n",
    "    y_pred = a + b * x + c * x ** 2 + d * x ** 3\n",
    "\n",
    "    # Compute and print loss using operations on Tensors.\n",
    "    # Now loss is a Tensor of shape (1,)\n",
    "    # loss.item() gets the scalar value held in the loss.\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "\n",
    "    # Calculare initial loss, so we can report loss relative to it\n",
    "    if t==0:\n",
    "        initial_loss=loss.item()\n",
    "\n",
    "    if t % 100 == 99:\n",
    "        print(f'Iteration t = {t:4d}  loss(t)/loss(0) = {round(loss.item()/initial_loss, 6):10.6f}  a = {a.item():10.6f}  b = {b.item():10.6f}  c = {c.item():10.6f}  d = {d.item():10.6f}')\n",
    "\n",
    "    # Use autograd to compute the backward pass. This call will compute the\n",
    "    # gradient of loss with respect to all Tensors with requires_grad=True.\n",
    "    # After this call a.grad, b.grad. c.grad and d.grad will be Tensors holding\n",
    "    # the gradient of the loss with respect to a, b, c, d respectively.\n",
    "    loss.backward()\n",
    "\n",
    "    # Manually update weights using gradient descent. Wrap in torch.no_grad()\n",
    "    # because weights have requires_grad=True, but we don't need to track this\n",
    "    # in autograd.\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} x + {c.item()} x^2 + {d.item()} x^3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "dfa26590",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99 209.95834350585938\n",
      "199 144.66018676757812\n",
      "299 100.70249938964844\n",
      "399 71.03519439697266\n",
      "499 50.97850799560547\n",
      "599 37.403133392333984\n",
      "699 28.206867218017578\n",
      "799 21.97318458557129\n",
      "899 17.745729446411133\n",
      "999 14.877889633178711\n",
      "1099 12.93176555633545\n",
      "1199 11.610918045043945\n",
      "1299 10.714258193969727\n",
      "1399 10.10548210144043\n",
      "1499 9.692106246948242\n",
      "1599 9.411375999450684\n",
      "1699 9.220745086669922\n",
      "1799 9.091286659240723\n",
      "1899 9.003362655639648\n",
      "1999 8.943641662597656\n",
      "Result: y = -2.9753338681715036e-10 + -2.208526849746704 * P3(-1.1693186696692948e-10 + 0.2554861009120941 x)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import math\n",
    "\n",
    "\n",
    "class LegendrePolynomial3(torch.autograd.Function):\n",
    "    \"\"\"\n",
    "    We can implement our own custom autograd Functions by subclassing\n",
    "    torch.autograd.Function and implementing the forward and backward passes\n",
    "    which operate on Tensors.\n",
    "    \"\"\"\n",
    "\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        \"\"\"\n",
    "        In the forward pass we receive a Tensor containing the input and return\n",
    "        a Tensor containing the output. ctx is a context object that can be used\n",
    "        to stash information for backward computation. You can cache tensors for\n",
    "        use in the backward pass using the ``ctx.save_for_backward`` method. Other\n",
    "        objects can be stored directly as attributes on the ctx object, such as\n",
    "        ``ctx.my_object = my_object``. Check out `Extending torch.autograd <https://docs.pytorch.org/docs/stable/notes/extending.html#extending-torch-autograd>`_\n",
    "        for further details.\n",
    "        \"\"\"\n",
    "        ctx.save_for_backward(input)\n",
    "        return 0.5 * (5 * input ** 3 - 3 * input)\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        \"\"\"\n",
    "        In the backward pass we receive a Tensor containing the gradient of the loss\n",
    "        with respect to the output, and we need to compute the gradient of the loss\n",
    "        with respect to the input.\n",
    "        \"\"\"\n",
    "        input, = ctx.saved_tensors\n",
    "        return grad_output * 1.5 * (5 * input ** 2 - 1)\n",
    "\n",
    "\n",
    "dtype = torch.float\n",
    "device = torch.device(\"cpu\")\n",
    "# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU\n",
    "\n",
    "# Create Tensors to hold input and outputs.\n",
    "# By default, requires_grad=False, which indicates that we do not need to\n",
    "# compute gradients with respect to these Tensors during the backward pass.\n",
    "x = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\n",
    "y = torch.sin(x)\n",
    "\n",
    "# Create random Tensors for weights. For this example, we need\n",
    "# 4 weights: y = a + b * P3(c + d * x), these weights need to be initialized\n",
    "# not too far from the correct result to ensure convergence.\n",
    "# Setting requires_grad=True indicates that we want to compute gradients with\n",
    "# respect to these Tensors during the backward pass.\n",
    "a = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "b = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\n",
    "c = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\n",
    "d = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)\n",
    "\n",
    "learning_rate = 5e-6\n",
    "for t in range(2000):\n",
    "    # To apply our Function, we use Function.apply method. We alias this as 'P3'.\n",
    "    P3 = LegendrePolynomial3.apply\n",
    "\n",
    "    # Forward pass: compute predicted y using operations; we compute\n",
    "    # P3 using our custom autograd operation.\n",
    "    y_pred = a + b * P3(c + d * x)\n",
    "\n",
    "    # Compute and print loss\n",
    "    loss = (y_pred - y).pow(2).sum()\n",
    "    if t % 100 == 99:\n",
    "        print(t, loss.item())\n",
    "\n",
    "    # Use autograd to compute the backward pass.\n",
    "    loss.backward()\n",
    "\n",
    "    # Update weights using gradient descent\n",
    "    with torch.no_grad():\n",
    "        a -= learning_rate * a.grad\n",
    "        b -= learning_rate * b.grad\n",
    "        c -= learning_rate * c.grad\n",
    "        d -= learning_rate * d.grad\n",
    "\n",
    "        # Manually zero the gradients after updating weights\n",
    "        a.grad = None\n",
    "        b.grad = None\n",
    "        c.grad = None\n",
    "        d.grad = None\n",
    "\n",
    "print(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1db91ec2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "class SignSTE(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, input):\n",
    "        return input.sign()\n",
    "\n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        # \"Straight-through estimator\":\n",
    "        # pretend derivative of sign(x) is 1\n",
    "        grad_input = grad_output.clone()\n",
    "        return grad_input"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7851987e",
   "metadata": {},
   "source": [
    "### creating a simple computational graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e9b28f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#x → (f1: *3) → (f2: sin) → (f3: +5) → (f4: square) → (f5: SignSTE) → (sum) → s\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "a = x * 3            # f1\n",
    "b = torch.sin(a)     # f2\n",
    "c = b + 5            # f3\n",
    "d = c * c            # f4\n",
    "y = SignSTE.apply(d) # f5 (custom)\n",
    "s = y.sum()          # final\n",
    "s.backward(retain_graph=True)\n",
    "x.grad.zero_()\n",
    "s.backward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "95f15f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor(4., grad_fn=<MulBackward0>),)\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**2\n",
    "grads = torch.autograd.grad(y, x, create_graph=True)\n",
    "print(grads)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "491a1ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First derivative dd/dx = 27.195388793945312\n",
      "Second derivative d²d/dx² = 40.33676528930664\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "\n",
    "# forward\n",
    "a = x * 3\n",
    "b = torch.sin(a)\n",
    "c = b + 5\n",
    "d = c * c  # final output\n",
    "\n",
    "# first derivative\n",
    "grad1 = torch.autograd.grad(d, x, create_graph=True)[0]\n",
    "print(\"First derivative dd/dx =\", grad1.item())\n",
    "\n",
    "# second derivative\n",
    "grad2 = torch.autograd.grad(grad1, x)[0]\n",
    "print(\"Second derivative d²d/dx² =\", grad2.item())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
