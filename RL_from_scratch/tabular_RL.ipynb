{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9c26fdca",
   "metadata": {},
   "source": [
    "# Tabular Reinforcement Learning (RL)\n",
    "\n",
    "Reinforcement Learning (RL) is a learning paradigm where an **agent** interacts with an **environment** by:\n",
    "- Observing the **state** $s \\in S$,\n",
    "- Taking an **action** $a \\in A$,\n",
    "- Receiving a **reward** $r \\in \\mathbb{R}$,\n",
    "- Transitioning to a new **state** $s'$.\n",
    "\n",
    "The goal is to learn a **policy** $\\pi(s)$ that maximizes the long-term reward.\n",
    "\n",
    "---\n",
    "\n",
    "## Markov Decision Process (MDP)\n",
    "\n",
    "An RL problem is typically modeled as an MDP, defined by:\n",
    "- **States**: $S$\n",
    "- **Actions**: $A$\n",
    "- **Reward function**: $R(s,a)$\n",
    "- **Transition probability**: $P(s' \\mid s,a)$\n",
    "- **Discount factor**: $\\gamma \\in [0,1]$\n",
    "\n",
    "---\n",
    "\n",
    "## Q-Learning (Tabular RL)\n",
    "\n",
    "Tabular RL stores knowledge in a **Q-table**, where each entry $Q(s,a)$ estimates the value of taking action $a$ in state $s$.\n",
    "\n",
    "**Update rule (Q-learning, off-policy):**\n",
    "\n",
    "$$\n",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha\\Big[ r + \\gamma \\max_{a'} Q(s',a') - Q(s,a) \\Big]\n",
    "$$\n",
    "\n",
    "- $\\alpha$: learning rate  \n",
    "- $\\gamma$: discount factor  \n",
    "- $r$: reward  \n",
    "- $s'$: next state  \n",
    "\n",
    "---\n",
    "\n",
    "## SARSA (On-policy Tabular RL)\n",
    "\n",
    "**Update rule (SARSA):**\n",
    "\n",
    "$$\n",
    "Q(s,a) \\leftarrow Q(s,a) + \\alpha\\Big[ r + \\gamma Q(s',a') - Q(s,a) \\Big]\n",
    "$$\n",
    "\n",
    "SARSA differs from Q-learning because it uses the action $a'$ actually taken by the policy in the next state (on-policy).\n",
    "\n",
    "---\n",
    "## Expected SARSA (On-policy Tabular RL)\n",
    "**Update rule (Expected SARSA):**\n",
    "$$\n",
    "Q(s, a) \\leftarrow Q(s, a) + \\alpha \\Big[r + \\gamma \\cdot \\frac{1}{n} \\sum_{a'} Q(s', a') - Q(s, a)\\Big]\n",
    "$$\n",
    "---\n",
    "\n",
    "*Author: **Samuel Boluwatife Giwa**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3d21762",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "alpha = 0.1        # learning rate\n",
    "gamma = 0.99       # discount factor\n",
    "epsilon = 0.3      # exploration probability\n",
    "episodes = 200    \n",
    "n_states = 16\n",
    "n_actions = 4\n",
    "\n",
    "grid = np.zeros((4, 4))\n",
    "\n",
    "Q = np.random.rand(n_states, n_actions)\n",
    "\n",
    "action = {'left': 0 , 'right': 1 ,'up':2,'down':3}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b112e94",
   "metadata": {},
   "outputs": [],
   "source": [
    "#environment dynamics\n",
    "#Grid like environment, index starts from 0\n",
    "\n",
    "def step(state, action):\n",
    "    column = state % 4\n",
    "    row = state // 4\n",
    "    column_ = column\n",
    "    row_ = row\n",
    "    \n",
    "    \n",
    "    if action == 0:   #left\n",
    "        if column_ != 0:\n",
    "            column_ -= 1\n",
    "    elif action == 1: #right\n",
    "        if column_ != 3:\n",
    "            column_ += 1\n",
    "    elif action == 2:  #up\n",
    "        if row_ != 0:\n",
    "            row_ -= 1\n",
    "    elif action == 3:   #down\n",
    "        if row_ != 3:\n",
    "            row_ += 1\n",
    "\n",
    "    new_position = ((row_)*4) + (column_)\n",
    "\n",
    "    return [row_ , column_, new_position]\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "da65bdc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reward function\n",
    "\n",
    "from typing import Any, Literal\n",
    "\n",
    "def reward(position_:int , goal_state: int) -> Literal[10, -1]:\n",
    "    \n",
    "    if position_ == goal_state:\n",
    "        reward = 10\n",
    "    else:\n",
    "        reward = -1\n",
    "    return reward\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91743c0",
   "metadata": {},
   "source": [
    "# Q-Learning Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dd28e3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "episodes = 500\n",
    "goal_state = 15\n",
    "epsilon = 0.1\n",
    "\n",
    "for i in range(episodes):\n",
    "    current_state = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Epsilon-greedy action selection\n",
    "        \n",
    "        \n",
    "        if np.random.rand() < epsilon:\n",
    "            current_action = np.random.randint(0, 4)\n",
    "        else:\n",
    "            current_action = np.argmax(Q[current_state, :4]) \n",
    "\n",
    "        row, col, next_state = step(current_state, current_action)\n",
    "        r = reward(next_state, goal_state)\n",
    "\n",
    "        # Q-learning update\n",
    "        best_next_action = np.argmax(Q[next_state, :])\n",
    "        Q[current_state, current_action] += alpha * (\n",
    "            r + gamma * Q[next_state, best_next_action] - Q[current_state, current_action]\n",
    "        )\n",
    "\n",
    "        current_state = next_state\n",
    "\n",
    "        if current_state == goal_state:\n",
    "            done = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a17e43fa",
   "metadata": {},
   "source": [
    "# SARSA Algorithm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c153e065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SARSA\n",
    "\n",
    "\n",
    "episodes = 500\n",
    "goal_state = 15\n",
    "epsilon = 0.1\n",
    "\n",
    "\n",
    "\n",
    "for i in range(episodes):\n",
    "    current_state = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Epsilon-greedy action selection\n",
    "        \n",
    "        \n",
    "        if np.random.rand() < epsilon:\n",
    "            current_action = np.random.randint(0, 4)\n",
    "        else:\n",
    "            current_action = np.argmax(Q[current_state, :4]) \n",
    "\n",
    "        row, col, next_state = step(current_state, current_action)\n",
    "        r = reward(next_state, goal_state)\n",
    "\n",
    "\n",
    "        if np.random.rand() < epsilon:\n",
    "            next_action = np.random.randint(0, 4)\n",
    "        else:\n",
    "            next_action = np.argmax(Q[next_state, :])\n",
    "        \n",
    "        \n",
    "        # SARSA update\n",
    "        best_next_action = np.argmax(Q[next_state, :])\n",
    "        Q[current_state, current_action] += alpha * (\n",
    "            r + gamma * Q[next_state, next_action] - Q[current_state, current_action]\n",
    "        )\n",
    "\n",
    "        current_state = next_state\n",
    "\n",
    "        if current_state == goal_state:\n",
    "            done = True\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b35705b",
   "metadata": {},
   "source": [
    "## EXPECTED SARSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1337acb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# EXPECTED SARSA\n",
    "\n",
    "episodes = 500\n",
    "goal_state = 15\n",
    "epsilon = 0.1\n",
    "n=4\n",
    "\n",
    "for i in range(episodes):\n",
    "    current_state = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Epsilon-greedy action selection\n",
    "        \n",
    "        \n",
    "        if np.random.rand() < epsilon:\n",
    "            current_action = np.random.randint(0, 4)\n",
    "        else:\n",
    "            current_action = np.argmax(Q[current_state, :4]) \n",
    "\n",
    "        row, col, next_state = step(current_state, current_action)\n",
    "        r = reward(next_state, goal_state)\n",
    "        \n",
    "        \n",
    "        #summation of Q values of next state over all posible actions\n",
    "        \n",
    "        sum_Q = np.sum(Q[next_state,:n])\n",
    "        \n",
    "        \n",
    "\n",
    "        # Q-learning update\n",
    "        best_next_action = np.argmax(Q[next_state, :])\n",
    "        Q[current_state, current_action] += alpha * (\n",
    "            r + gamma * ((1-epsilon)*Q[next_state, best_next_action] + epsilon/n * sum_Q )- Q[current_state, current_action]\n",
    "        )\n",
    "\n",
    "        current_state = next_state\n",
    "\n",
    "        if current_state == goal_state:\n",
    "            done = True\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11db8a44",
   "metadata": {},
   "source": [
    "## DOUBLE Q-LEARNING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b3f5ebe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# double Q learning\n",
    "\n",
    "\n",
    "episodes = 500\n",
    "goal_state = 15\n",
    "epsilon = 0.1\n",
    "Q1 = np.random.rand(n_states, n_actions)\n",
    "Q2 = np.random.rand(n_states, n_actions)\n",
    "\n",
    "for i in range(episodes):\n",
    "    current_state = 0\n",
    "    done = False\n",
    "\n",
    "    while not done:\n",
    "        # Epsilon-greedy action selection\n",
    "        \n",
    "        \n",
    "        if np.random.rand() < epsilon:\n",
    "            current_action = np.random.randint(0, 4)\n",
    "        else:\n",
    "            current_action = np.argmax(Q1[current_state, :] + Q2[current_state, :])\n",
    "            \n",
    "            \n",
    "        row, col, next_state = step(current_state, current_action)\n",
    "        r = reward(next_state, goal_state)\n",
    "        \n",
    "        if np.random.rand() < 0.5:\n",
    "            best_next_action = np.argmax(Q1[next_state, :])\n",
    "            Q1[current_state, current_action] += alpha * (\n",
    "            r + gamma * Q2[next_state, best_next_action] - Q1[current_state, current_action]\n",
    "        )\n",
    "        else:\n",
    "           best_next_action = np.argmax(Q2[next_state, :])\n",
    "           Q2[current_state, current_action] += alpha * (\n",
    "            r + gamma * Q1[next_state, best_next_action] - Q2[current_state, current_action]\n",
    "        )\n",
    "\n",
    "        current_state = next_state\n",
    "\n",
    "        if current_state == goal_state:\n",
    "            done = True\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
